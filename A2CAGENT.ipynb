{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BpRwx55jfu8H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf. __version__)\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from networks import ActorCriticNetwork\n",
    "from NetworkEnv import NetworkEnv as Ne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcphR4HP9g_z",
    "outputId": "1be10f11-a831-4349-b095-6fb01d5a7d6c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "train = []                            ## CREATING TRAIN SET OF 1000 GAMES\n",
    "for i in range(1000):\n",
    "  q1 = np.random.randint(2,6, 7)\n",
    "  q2 = np.random.randint(1,6,7)\n",
    "  q3 = np.random.randint(1,6,7)\n",
    "  q4= np.random.randint(1,6,7)\n",
    "  q5= np.random.randint(1,5,7)\n",
    "\n",
    "  z = [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]\n",
    "  for i in range(len(q2)):\n",
    "    if q2[i]==2:\n",
    "      q2[i]+=1\n",
    "  for i in range(len(q3)):\n",
    "    if q3[i]==3:\n",
    "      q3[i]+=1\n",
    "  for i in range(len(q4)):\n",
    "    if q4[i]==4:\n",
    "      q4[i]+=1\n",
    "\n",
    "\n",
    "  data = np.array([q1,q2,q3,q4,q5]).T\n",
    "  data = np.vstack([data,z])\n",
    "  df = pd.DataFrame(data, columns=[\"N1Q\",\"N2Q\",\"N3Q\",\"N4Q\",\"N5Q\"])\n",
    "  train.append(df)\n",
    "\n",
    "print(len(train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "30CrgVSTicmz"
   },
   "outputs": [],
   "source": [
    "def preprocess(new_state):            ## Preprocess state to turn into one hot state to be fed to Network\n",
    "  q1 = new_state[:,0]\n",
    "  q2 = new_state[:,1]\n",
    "  q3 = new_state[:,2]\n",
    "  q4 = new_state[:,3]\n",
    "  q5 = new_state[:,4]\n",
    "  q1 =np.array(q1)\n",
    "  ohq1 = np.zeros((q1.shape[0], 6))\n",
    "  ohq1[np.arange(q1.size),q1]=1\n",
    "\n",
    "  q2 =np.array(q2)\n",
    "  ohq2 = np.zeros((q2.shape[0], 6))\n",
    "  ohq2[np.arange(q2.size),q2]=1\n",
    "  \n",
    "\n",
    "  q3 =np.array(q3)\n",
    "  ohq3 = np.zeros((q3.shape[0], 6))\n",
    "  ohq3[np.arange(q3.size),q3]=1\n",
    "\n",
    "  q4 =np.array(q4)\n",
    "  ohq4 = np.zeros((q4.shape[0], 6))\n",
    "  ohq4[np.arange(q4.size),q4]=1\n",
    "\n",
    "  q5 =np.array(q5)\n",
    "  ohq5 = np.zeros((q5.shape[0], 6))\n",
    "  ohq5[np.arange(q5.size),q5]=1\n",
    "\n",
    "  ns = np.dstack((ohq1,ohq2,ohq3,ohq4,ohq5))\n",
    "  return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fn9tbBXYyKZZ"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env_name, alpha=0.00025, gamma=0.99, n_actions=[4,4,4,4,4]):\n",
    "        self.gamma = gamma\n",
    "        self.action_space = n_actions\n",
    "        self.action = None\n",
    "        self.actor_critic = ActorCriticNetwork(action_space=n_actions)\n",
    "        self.env = Ne(env_name)\n",
    "\n",
    "\n",
    "\n",
    "        self.actor_critic.compile(optimizer=Adam(learning_rate=alpha))\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        _, probs = self.actor_critic(state)\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=probs)\n",
    "        action = action_probabilities.sample()\n",
    "        log_prob = action_probabilities.log_prob(action)\n",
    "        self.action = action\n",
    "        print(action)\n",
    "\n",
    "        return action.numpy()\n",
    "\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor_critic.save_weights(self.actor_critic.checkpoint_file)\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor_critic.load_weights(self.actor_critic.checkpoint_file)\n",
    "\n",
    "    def reset(self,df):\n",
    "        self.env.reset(df)\n",
    "        state = self.env.state_observation\n",
    "        done = self.env.done\n",
    "        reward = self.env.reward\n",
    "        return state, done,reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, state, reward, state_, done):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)  # not fed to NN\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            state_value, probs = self.actor_critic(state)\n",
    "            state_value_, _ = self.actor_critic(state_)\n",
    "            state_value = tf.squeeze(state_value)\n",
    "            state_value_ = tf.squeeze(state_value_)\n",
    "\n",
    "            action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "            log_prob = action_probs.log_prob(self.action)\n",
    "\n",
    "            delta = reward + self.gamma * state_value_ * (1 - int(done)) - state_value\n",
    "            actor_loss = -log_prob * delta\n",
    "            critic_loss = delta ** 2\n",
    "            total_loss = actor_loss + critic_loss\n",
    "\n",
    "        gradient = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
    "        self.actor_critic.optimizer.apply_gradients(zip(\n",
    "            gradient, self.actor_critic.trainable_variables))\n",
    "        \n",
    "\n",
    "    def run(self, df):\n",
    "        # uncomment this line and do a mkdir tmp && mkdir video if you want to\n",
    "        # record video of the agent playing the game.\n",
    "        # env = wrappers.Monitor(env, 'tmp/video', video_callable=lambda episode_id: True, force=True)\n",
    "        filename = 'results.png'\n",
    "\n",
    "        figure_file = 'plots/' + filename\n",
    "\n",
    "        best_score = -1000\n",
    "        score_history = []\n",
    "        load_checkpoint = False\n",
    "\n",
    "        if load_checkpoint:\n",
    "            self.load_models()\n",
    "\n",
    "        for i in df:\n",
    "            observation,done, score = self.reset(i)\n",
    "            count = 0\n",
    "\n",
    "            while not done:\n",
    "                ohtob = preprocess(observation)\n",
    "                action = self.choose_action(ohtob)\n",
    "                observation_, reward, done, info = self.env.step(action, observation)\n",
    "                score += reward\n",
    "                ohtob_ = preprocess(observation_)\n",
    "                print(score)\n",
    "                print(action)\n",
    "                print(observation)\n",
    "                if not load_checkpoint:\n",
    "                    self.learn(ohtob, reward, ohtob_, done)\n",
    "                observation = observation_\n",
    "                score_history.append(score)\n",
    "                avg_score = np.mean(score_history[-100:])\n",
    "                print(done)\n",
    "                count +=1\n",
    "                if count >200:\n",
    "                  done = True\n",
    "                  score -= 500\n",
    "                \n",
    "            \n",
    "\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                if not load_checkpoint:\n",
    "                    self.save_models()\n",
    "\n",
    "            #print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\n",
    "\n",
    "        if not load_checkpoint:\n",
    "          N = len(score_history)\n",
    "          running_avg = np.empty(N)\n",
    "          for t in range(N):\n",
    "            running_avg[t] = np.mean(scores[max(0, t - window):(t + 1)])\n",
    "            if x is None:\n",
    "              x = [i for i in range(N)]\n",
    "            plt.ylabel('Score')\n",
    "            plt.xlabel('Game')\n",
    "            plt.plot(x, running_avg)\n",
    "            plt.savefig(filename)\n",
    "\n",
    "    def test(self, Model_name,df):\n",
    "        self.load(Model_name)\n",
    "        for e in range(100):\n",
    "          state = self.reset(df)\n",
    "          done = False\n",
    "          score = 0\n",
    "          while not done:\n",
    "            print(done)\n",
    "            one_hot_state = preprocess(state)\n",
    "            one_hot_state = tf.convert_to_tensor([one_hot_state]) \n",
    "            prediction = self.Actor.predict(one_hot_state)\n",
    "            action = np.zeros(5)\n",
    "            count = 0\n",
    "            for i in prediction:\n",
    "              for j in i:\n",
    "                action[count] = np.argmax(j)\n",
    "                count +=1\n",
    "            state, reward, done, _ = self.step(action)\n",
    "            score += reward\n",
    "            print(score)\n",
    " \n",
    "          if done:\n",
    "            print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "            print(average)\n",
    "            break\n",
    "        \n",
    "      # close environemnt when finish training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rqvJgUdjtgjk",
    "outputId": "9b492a3c-10ff-4175-ec82-7745a7c49b42"
   },
   "outputs": [],
   "source": [
    "    env_name = ''\n",
    "    agent = Agent(env_name)\n",
    "    #agent.run(train)                      ## Running train sequence for agent on train set     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "OMC1u2lbu_Kc",
    "outputId": "0fafaa97-3639-40ee-861f-87f77783a471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/2000, score: -3753\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-a74f13352713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Models/Nettest1_PG_1.h5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-124-32594e01300c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, Model_name, df)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode: {}/{}, score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m               \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'average' is not defined"
     ]
    }
   ],
   "source": [
    "agent.test(\"/content/Models/Nettest1_PG_1.h5\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "L5zElLzhCakM",
    "outputId": "a4138acb-5a24-428b-dcec-198b80305b12"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-c81d51f1e861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplotLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigure_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score_history' is not defined"
     ]
    }
   ],
   "source": [
    "x = [i + 1 for i in range(1000)]\n",
    "plotLearning(x, self.score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQkzxSFHUH-q",
    "outputId": "02091ef7-abaf-4cf3-e5aa-f3046b093fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   N1Q  N2Q  N3Q  N4Q  N5Q\n",
      "0    3    5    5    5    3\n",
      "1    5    4    2    1    3\n",
      "2    2    3    2    2    3\n",
      "3    4    5    1    5    3\n",
      "4    4    3    4    3    1\n",
      "5    4    1    4    3    4\n",
      "6    5    4    2    5    2\n",
      "7    0    0    0    0    0\n",
      "8    0    0    0    0    0\n",
      "9    0    0    0    0    0\n"
     ]
    }
   ],
   "source": [
    "print(train[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYuujEBIK8Tz",
    "outputId": "a52c21bf-6a7f-4b2f-a62d-1e46e5d794b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading models ...\n"
     ]
    }
   ],
   "source": [
    "agent.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "kyHSlrGrL4jC",
    "outputId": "64cbfa9d-aaa4-4f25-f47f-17478a75f075"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e04c202c6d41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'score_history'"
     ]
    }
   ],
   "source": [
    "print(agent.score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "tpQr6_mjMzLX",
    "outputId": "8d4533a4-1bee-4568-9eee-becb56b3b91c"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-61dfca954fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-123-27e04c21aadf>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-27e04c21aadf>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/NetworkEnv.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Reset the state of the environment to an initial state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode_1Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"N1Q\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode_2Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"N2Q\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode_3Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"N3Q\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "agent.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9c0iaVNNBNS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "A2CAGENT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
