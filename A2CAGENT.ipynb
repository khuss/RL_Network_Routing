{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2CAGENT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpRwx55jfu8H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import gym\n",
        "import pylab\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras import backend as K\n",
        "import cv2\n",
        "from NetworkEnv import NetworkEnv as Ne"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"name\"\n",
        "test = Ne(x)\n",
        "print(type(test.state_observation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-Pd2Plcf6Ig",
        "outputId": "1cf354d8-edf6-4e39-b839-90879908bec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([[3,4,1,2,3],[2,5,1,5,1],[3,5,2,5,3],[4,3,5,1,2],[4,3,2,1,4],[2,1,4,5,3],[5,5,2,2,1],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]])\n",
        "df = pd.DataFrame(data, columns=[\"N1Q\",\"N2Q\",\"N3Q\",\"N4Q\",\"N5Q\"])"
      ],
      "metadata": {
        "id": "gxUnQIKUf77U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.reset(df)\n",
        "print(test.state_observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvAFSDNngBY6",
        "outputId": "e09545ac-e28f-468f-cf26-6ef0493b086e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3 4 1 2 3]\n",
            " [2 5 1 5 1]\n",
            " [3 5 2 5 3]\n",
            " [4 3 5 1 2]\n",
            " [4 3 2 1 4]\n",
            " [2 1 4 5 3]\n",
            " [5 5 2 2 1]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_state = preprocess(test.state_observation)\n",
        "print(one_hot_state)\n",
        "print(one_hot_state.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyNO1GW1gDaN",
        "outputId": "31b1107e-82a1-4e10-88b0-e82b98fcff4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 1. 0.]\n",
            "  [1. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 1.]\n",
            "  [1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0.]\n",
            "  [1. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0.]\n",
            "  [0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1.]\n",
            "  [0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1.]\n",
            "  [0. 0. 1. 1. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [1. 1. 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. 1. 1.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. 1. 1.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. 1. 1.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0.]]]\n",
            "(10, 6, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testactor, testcritic  = OurModel([10,6,5], [3,4,4,4,4], 0.00025)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f90fO9y4UOP",
        "outputId": "3ba9745b-1c41-4fa6-9346-a491d5ca7364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 6, 5), dtype=tf.float32, name='input_17'), name='input_17', description=\"created by layer 'input_17'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state =  tf.convert_to_tensor([one_hot_state])\n",
        "prob = testactor.predict(state) \n",
        "action = np.zeros(5)\n",
        "count = 0\n",
        "for i in prob:\n",
        "  print(i)\n",
        "  for j in i:\n",
        "    print(j)\n",
        "    action[count] = np.random.choice(len(j),p=j)\n",
        "    count +=1\n",
        "print(action)\n",
        "values = testcritic.predict(state)\n",
        "for i in values:\n",
        "  print(i[:,0])\n",
        "  for j in i:\n",
        "    print(j[0])\n",
        "\n",
        "print(values)\n",
        "c = preprocess_action(action)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "njlf4VLn4lH1",
        "outputId": "933ab7b9-7990-474f-f4a5-cb1c792752a4"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.37103254 0.3275475  0.30142003]]\n",
            "[0.37103254 0.3275475  0.30142003]\n",
            "[[0.4119788  0.16134168 0.2126781  0.21400145]]\n",
            "[0.4119788  0.16134168 0.2126781  0.21400145]\n",
            "[[0.13933189 0.25303352 0.2200966  0.38753796]]\n",
            "[0.13933189 0.25303352 0.2200966  0.38753796]\n",
            "[[0.2045208  0.19550997 0.19996275 0.40000647]]\n",
            "[0.2045208  0.19550997 0.19996275 0.40000647]\n",
            "[[0.37628692 0.20249857 0.20806566 0.2131488 ]]\n",
            "[0.37628692 0.20249857 0.20806566 0.2131488 ]\n",
            "[1. 2. 0. 3. 0.]\n",
            "[-0.47134167]\n",
            "-0.47134167\n",
            "[0.17622513]\n",
            "0.17622513\n",
            "[-0.3048492]\n",
            "-0.3048492\n",
            "[0.18241327]\n",
            "0.18241327\n",
            "[0.05048151]\n",
            "0.050481513\n",
            "[array([[-0.47134167]], dtype=float32), array([[0.17622513]], dtype=float32), array([[-0.3048492]], dtype=float32), array([[0.18241327]], dtype=float32), array([[0.05048151]], dtype=float32)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-24e540e4b198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-b9ebbdc05808>\u001b[0m in \u001b[0;36mpreprocess_action\u001b[0;34m(action)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0moha1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0moha1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0moha2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(new_state):\n",
        "  q1 = new_state[:,0]\n",
        "  q2 = new_state[:,1]\n",
        "  q3 = new_state[:,2]\n",
        "  q4 = new_state[:,3]\n",
        "  q5 = new_state[:,4]\n",
        "  q1 =np.array(q1)\n",
        "  ohq1 = np.zeros((q1.shape[0], 6))\n",
        "  ohq1[np.arange(q1.size),q1]=1\n",
        "\n",
        "  q2 =np.array(q2)\n",
        "  ohq2 = np.zeros((q2.shape[0], 6))\n",
        "  ohq2[np.arange(q2.size),q2]=1\n",
        "  \n",
        "\n",
        "  q3 =np.array(q3)\n",
        "  ohq3 = np.zeros((q3.shape[0], 6))\n",
        "  ohq3[np.arange(q3.size),q3]=1\n",
        "\n",
        "  q4 =np.array(q4)\n",
        "  ohq4 = np.zeros((q4.shape[0], 6))\n",
        "  ohq4[np.arange(q4.size),q4]=1\n",
        "\n",
        "  q5 =np.array(q5)\n",
        "  ohq5 = np.zeros((q5.shape[0], 6))\n",
        "  ohq5[np.arange(q5.size),q5]=1\n",
        "\n",
        "  ns = np.dstack((ohq1,ohq2,ohq3,ohq4,ohq5))\n",
        "  return ns\n",
        "\n",
        "def preprocess_action(action):\n",
        "  a1=action[0]\n",
        "  a2=action[1]\n",
        "  a3=action[2]\n",
        "  a4=action[3]\n",
        "  a5=action[4]\n",
        "\n",
        "  oha1=np.zeros(3)\n",
        "  oha1[a1] = 1\n",
        "\n",
        "  oha2=np.zeros(4)\n",
        "  oha2[a2] = 1\n",
        "\n",
        "  oha3=np.zeros(4)\n",
        "  oha3[a3] = 1\n",
        "\n",
        "  oha4=np.zeros(4)\n",
        "  oha4[a4] = 1\n",
        "\n",
        "  oha5 = np.zeros(4)\n",
        "  oha5[a5] = 1\n",
        "\n",
        "  return ([a1,a2,a3,a4,a5])\n",
        "\n",
        "\n",
        "\n",
        "def OurModel(state_shape, action_space, lr):\n",
        "  X_input = Input(state_shape)\n",
        "  print(X_input)\n",
        "  X = Conv2D(32, 3, activation = \"relu\", input_shape=(state_shape))(X_input)\n",
        "  X = MaxPooling2D((2,2))(X)\n",
        "  X = Flatten()(X)\n",
        "\n",
        "  action_1 = Dense(action_space[0], activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
        "  action_2 = Dense(action_space[1], activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
        "  action_3 = Dense(action_space[2], activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
        "  action_4 = Dense(action_space[3], activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
        "  action_5 = Dense(action_space[4], activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
        "\n",
        "  value_1 = Dense(1, kernel_initializer='he_uniform')(X)\n",
        "  value_2 = Dense(1, kernel_initializer='he_uniform')(X)\n",
        "  value_3 = Dense(1, kernel_initializer='he_uniform')(X)\n",
        "  value_4 = Dense(1, kernel_initializer='he_uniform')(X)\n",
        "  value_5 = Dense(1, kernel_initializer='he_uniform')(X)\n",
        "\n",
        "  Actor = Model(inputs = X_input, outputs = [action_1, action_2, action_3, action_4, action_5])  \n",
        "  Actor.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr))\n",
        "\n",
        "\n",
        "  Critic = Model(inputs = X_input, outputs = [value_1, value_2, value_3, value_4, value_5])\n",
        "  Critic.compile(loss='mse', optimizer=Adam(lr=lr))\n",
        "\n",
        "\n",
        "\n",
        "  return Actor, Critic\n",
        "\n"
      ],
      "metadata": {
        "id": "30CrgVSTicmz"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class A2CAgent:\n",
        "    # Policy Gradient Main Optimization Algorithm\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name       \n",
        "        self.env = Ne(env_name)\n",
        "        self.action_size = [self.env.N1_max_actions+1, self.env.N2_max_actions+1, self.env.N3_max_actions+1, self.env.N4_max_actions+1, self.env.N5_max_actions+1]\n",
        "        self.EPISODES, self.max_average = 2000, -1000000.0\n",
        "        self.lr = 1\n",
        "\n",
        "        self.max_Q = 10\n",
        "        self.Nodes = 5\n",
        "        self.one_hot_dim = 6\n",
        "\n",
        "        self.Save_Path = 'Models'\n",
        "\n",
        "        \n",
        "        self.states, self.actions, self.rewards = [], [], []\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        self.state_size = [self.max_Q, self.one_hot_dim, self.Nodes]\n",
        "        \n",
        "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
        "        self.path = '{}_PG_{}'.format(self.env_name, self.lr)\n",
        "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
        "\n",
        "\n",
        "        self.Actor, self.Critic = OurModel(self.state_size, self.action_size, self.lr)\n",
        "\n",
        "\n",
        "    def load(self, Actor_name):\n",
        "        self.Actor = load_model(Actor_name, compile=False)\n",
        "\n",
        "    def save(self):\n",
        "        print(self.Model_name)\n",
        "        self.Actor.save(self.Model_name + '.h5')\n",
        "        \n",
        "\n",
        "\n",
        "    def remember(self, state, action, reward):\n",
        "        # store episode actions to memory\n",
        "        one_hot_state = preprocess(state)\n",
        "        one_hot_state = tf.convert_to_tensor([one_hot_state]) \n",
        "        self.states.append(one_hot_state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        " \n",
        "\n",
        "    def act(self, state):\n",
        "        # Use the network to predict the next action to take, using the model\n",
        "        one_hot_state = preprocess(state)\n",
        "        one_hot_state = tf.convert_to_tensor([one_hot_state]) \n",
        "        prediction = self.Actor.predict(one_hot_state)\n",
        "        action = np.zeros(5)\n",
        "        count = 0\n",
        "        for i in prediction:\n",
        "          for j in i:\n",
        "            action[count] = np.random.choice(len(j),p=j)\n",
        "            count +=1\n",
        "        return action\n",
        "\n",
        "\n",
        "    def replay(self):\n",
        "        # reshape memory to appropriate shape for training\n",
        "        states = np.vstack(self.states)\n",
        "        actions = np.vstack(self.actions)\n",
        "\n",
        "\n",
        "        # Compute discounted rewards\n",
        "        discounted_r = self.discount_rewards(self.rewards)\n",
        "\n",
        "        # Get Critic network predictions\n",
        "        crit = self.Critic.predict(states)\n",
        "        values = np.zeros(states.shape[0])\n",
        "        count = 0\n",
        "        for i in crit:\n",
        "          count+=1\n",
        "          for j in i:\n",
        "            values[count] = j[0]\n",
        "        # Compute advantages\n",
        "        advantages = discounted_r - values\n",
        "        # training Actor and Critic networks\n",
        "        self.Actor.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
        "        self.Critic.fit(states, discounted_r, epochs=1, verbose=0)\n",
        "        # reset training memory\n",
        "        self.states, self.actions, self.rewards = [], [], []\n",
        "\n",
        "    def discount_rewards(self, reward):\n",
        "    # Compute the gamma-discounted rewards over an episode\n",
        "        gamma = 0.99    # discount rate\n",
        "        running_add = 0\n",
        "        discounted_r = np.zeros_like(reward)\n",
        "        for i in reversed(range(0,len(reward))):\n",
        "            running_add = running_add * gamma + reward[i]\n",
        "            discounted_r[i] = running_add\n",
        "\n",
        "        discounted_r = discounted_r - np.mean(discounted_r) # normalizing the result\n",
        "        discounted_r = discounted_r/np.std(discounted_r) # divide by standard deviation\n",
        "        return discounted_r\n",
        "   \n",
        "    def PlotModel(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
        "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
        "            pylab.plot(self.episodes, self.scores, 'b')\n",
        "            pylab.plot(self.episodes, self.average, 'r')\n",
        "            pylab.ylabel('Score', fontsize=18)\n",
        "            pylab.xlabel('Steps', fontsize=18)\n",
        "            try:\n",
        "                pylab.savefig(self.path+\".png\")\n",
        "            except OSError:\n",
        "                pass\n",
        "\n",
        "        return self.average[-1]\n",
        "\n",
        "    def reset(self, df):\n",
        "        self.env.reset(df)\n",
        "        state = self.env.state_observation\n",
        "        return state\n",
        "\n",
        "    def step(self,action):\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def run(self, df):\n",
        "\n",
        "        for e in range(self.EPISODES):\n",
        "          state = self.reset(df)\n",
        "          done, score, SAVING = False, 0, ''\n",
        "          while not done:\n",
        "              #self.env.render()\n",
        "              # Actor picks an action\n",
        "              action = self.act(state)\n",
        "              # Retrieve new state, reward, and whether the state is terminal\n",
        "              next_state, reward, done, e = self.step(action)\n",
        "              # Memorize (state, action, reward) for training\n",
        "              self.remember(state, action, reward)\n",
        "              # Update current state\n",
        "              state = next_state\n",
        "              score += reward\n",
        "              \n",
        "              if done:\n",
        "                  average = self.PlotModel(score, self.env.episode_length)\n",
        "                  # saving best models\n",
        "                  if average >= self.max_average:\n",
        "                      self.max_average = average\n",
        "                      self.save()\n",
        "                      SAVING = \"SAVING\"\n",
        "                  else:\n",
        "                      SAVING = \"\"\n",
        "                  print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
        "\n",
        "                  self.replay()\n",
        "\n",
        "    def test(self, Model_name,df):\n",
        "        self.load(Model_name)\n",
        "        for e in range(100):\n",
        "          state = self.reset(df)\n",
        "          done = False\n",
        "          score = 0\n",
        "          while not done:\n",
        "            one_hot_state = preprocess(state)\n",
        "            one_hot_state = tf.convert_to_tensor([one_hot_state]) \n",
        "            prediction = self.Actor.predict(one_hot_state)\n",
        "            action = np.zeros(5)\n",
        "            count = 0\n",
        "            for i in prediction:\n",
        "              for j in i:\n",
        "                action[count] = np.argmax(j)\n",
        "                count +=1\n",
        "            state, reward, done, _ = self.step(action)\n",
        "            score += reward\n",
        " \n",
        "            if done:\n",
        "              print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
        "              print(average)\n",
        "              break\n",
        "        \n",
        "      # close environemnt when finish training\n"
      ],
      "metadata": {
        "id": "Fn9tbBXYyKZZ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    env_name = 'Nettest1'\n",
        "    agent = A2CAgent(env_name)\n",
        "    agent.run(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "rqvJgUdjtgjk",
        "outputId": "364da3eb-3094-4b0e-c5b4-1f8ff9ee6339"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 6, 5), dtype=tf.float32, name='input_42'), name='input_42', description=\"created by layer 'input_42'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models/Nettest1_PG_1\n",
            "episode: 47/2000, score: -861, average: -861.00 SAVING\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-7fd886f96395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Nettest1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA2CAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-32594e01300c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    151\u001b[0m                   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode: {}/{}, score: {}, average: {:.2f} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-32594e01300c>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscounted_r\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# training Actor and Critic networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscounted_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# reset training memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1790, in categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 3) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.test(\"/content/Models/Nettest1_PG_1.h5\",df)"
      ],
      "metadata": {
        "id": "OMC1u2lbu_Kc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "0fafaa97-3639-40ee-861f-87f77783a471"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0/2000, score: -3753\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-a74f13352713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Models/Nettest1_PG_1.h5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-32594e01300c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, Model_name, df)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode: {}/{}, score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m               \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'average' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,30):\n",
        "  agent.run(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "L5zElLzhCakM",
        "outputId": "ab9403f8-2967-4c59-b1c1-a3f7e0474ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 110/2000, score: -970, average: -1048.00 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-79a931cf724d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-b0c1257654ec>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    128\u001b[0m                   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"episode: {}/{}, score: {}, average: {:.2f} {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m       \u001b[0;31m# close environemnt when finish training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b0c1257654ec>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Compute discounted rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mdiscounted_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Get Critic network predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b0c1257654ec>\u001b[0m in \u001b[0;36mdiscount_rewards\u001b[0;34m(self, reward)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mdiscounted_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mdiscounted_r\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscounted_r\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# normalizing the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mdiscounted_r\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscounted_r\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# divide by standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdiscounted_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mQkzxSFHUH-q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}