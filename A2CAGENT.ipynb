{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BpRwx55jfu8H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "from NetworkEnv import NetworkEnv as Ne\n",
    "import matplotlib.pyplot as plt\n",
    "from networks import ActorCriticNetwork\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcphR4HP9g_z",
    "outputId": "1be10f11-a831-4349-b095-6fb01d5a7d6c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "train = []                            ## CREATING TRAIN SET OF 3000 GAMES\n",
    "for i in range(3000):\n",
    "  q1 = np.random.randint(2,6, 7)\n",
    "  q2 = np.random.randint(1,6,7)\n",
    "  q3 = np.random.randint(1,6,7)\n",
    "  q4= np.random.randint(1,6,7)\n",
    "  q5= np.random.randint(1,5,7)\n",
    "\n",
    "  z = [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]\n",
    "  for i in range(len(q2)):\n",
    "    if q2[i]==2:\n",
    "      q2[i]+=1\n",
    "  for i in range(len(q3)):\n",
    "    if q3[i]==3:\n",
    "      q3[i]+=1\n",
    "  for i in range(len(q4)):\n",
    "    if q4[i]==4:\n",
    "      q4[i]+=1\n",
    "\n",
    "\n",
    "  data = np.array([q1,q2,q3,q4,q5]).T\n",
    "  data = np.vstack([data,z])\n",
    "  df = pd.DataFrame(data, columns=[\"N1Q\",\"N2Q\",\"N3Q\",\"N4Q\",\"N5Q\"])\n",
    "  train.append(df)\n",
    "\n",
    "print(len(train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "30CrgVSTicmz"
   },
   "outputs": [],
   "source": [
    "def preprocess(new_state):            ## Preprocess state to turn into one hot state to be fed to Network\n",
    "  q1 = new_state[:,0]\n",
    "  q2 = new_state[:,1]\n",
    "  q3 = new_state[:,2]\n",
    "  q4 = new_state[:,3]\n",
    "  q5 = new_state[:,4]\n",
    "  q1 =np.array(q1)\n",
    "  ohq1 = np.zeros((q1.shape[0], 6))\n",
    "  ohq1[np.arange(q1.size),q1]=1\n",
    "\n",
    "  q2 =np.array(q2)\n",
    "  ohq2 = np.zeros((q2.shape[0], 6))\n",
    "  ohq2[np.arange(q2.size),q2]=1\n",
    "  \n",
    "\n",
    "  q3 =np.array(q3)\n",
    "  ohq3 = np.zeros((q3.shape[0], 6))\n",
    "  ohq3[np.arange(q3.size),q3]=1\n",
    "\n",
    "  q4 =np.array(q4)\n",
    "  ohq4 = np.zeros((q4.shape[0], 6))\n",
    "  ohq4[np.arange(q4.size),q4]=1\n",
    "\n",
    "  q5 =np.array(q5)\n",
    "  ohq5 = np.zeros((q5.shape[0], 6))\n",
    "  ohq5[np.arange(q5.size),q5]=1\n",
    "\n",
    "  ns = np.dstack((ohq1,ohq2,ohq3,ohq4,ohq5))\n",
    "  return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Fn9tbBXYyKZZ"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env_name, alpha=0.00025, gamma=0.99, n_actions=[4,4,4,4,4]):\n",
    "        self.gamma = gamma\n",
    "        self.action_space = n_actions\n",
    "        self.action = None\n",
    "        self.actor_critic = ActorCriticNetwork(action_space=n_actions)\n",
    "        self.env = Ne(env_name)\n",
    "\n",
    "\n",
    "\n",
    "        self.actor_critic.compile(optimizer=Adam(learning_rate=alpha))\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        _, probs = self.actor_critic(state)\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=probs)\n",
    "        action = action_probabilities.sample()\n",
    "        log_prob = action_probabilities.log_prob(action)\n",
    "        self.action = action\n",
    "        print(action)\n",
    "\n",
    "        return action.numpy()\n",
    "\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor_critic.save_weights(self.actor_critic.checkpoint_file)\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor_critic.load_weights(self.actor_critic.checkpoint_file)\n",
    "\n",
    "    def reset(self,df):\n",
    "        self.env.reset(df)\n",
    "        state = self.env.state_observation\n",
    "        done = self.env.done\n",
    "        reward = self.env.reward\n",
    "        return state, done,reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, state, reward, state_, done):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)  # not fed to NN\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            state_value, probs = self.actor_critic(state)\n",
    "            state_value_, _ = self.actor_critic(state_)\n",
    "            state_value = tf.squeeze(state_value)\n",
    "            state_value_ = tf.squeeze(state_value_)\n",
    "\n",
    "            action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "            log_prob = action_probs.log_prob(self.action)\n",
    "\n",
    "            delta = reward + self.gamma * state_value_ * (1 - int(done)) - state_value\n",
    "            actor_loss = -log_prob * delta\n",
    "            critic_loss = delta ** 2\n",
    "            total_loss = actor_loss + critic_loss\n",
    "\n",
    "        gradient = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
    "        self.actor_critic.optimizer.apply_gradients(zip(\n",
    "            gradient, self.actor_critic.trainable_variables))\n",
    "        \n",
    "\n",
    "    def run(self, df):\n",
    "        # uncomment this line and do a mkdir tmp && mkdir video if you want to\n",
    "        # record video of the agent playing the game.\n",
    "        # env = wrappers.Monitor(env, 'tmp/video', video_callable=lambda episode_id: True, force=True)\n",
    "        filename = 'results.png'\n",
    "\n",
    "        figure_file = 'plots/' + filename\n",
    "\n",
    "        best_score = -1000\n",
    "        score_history = []\n",
    "        load_checkpoint = False\n",
    "\n",
    "        if load_checkpoint:\n",
    "            self.load_models()\n",
    "\n",
    "        for i in df:\n",
    "            observation,done, score = self.reset(i)\n",
    "            count = 0\n",
    "\n",
    "            while not done:\n",
    "                ohtob = preprocess(observation)\n",
    "                action = self.choose_action(ohtob)\n",
    "                observation_, reward, done, info = self.env.step(action, observation)\n",
    "                score += reward\n",
    "                ohtob_ = preprocess(observation_)\n",
    "                print(score)\n",
    "                print(action)\n",
    "                print(observation)\n",
    "                if not load_checkpoint:\n",
    "                    self.learn(ohtob, reward, ohtob_, done)\n",
    "                observation = observation_\n",
    "                score_history.append(score)\n",
    "                avg_score = np.mean(score_history[-100:])\n",
    "                print(done)\n",
    "                count +=1\n",
    "                if count >200:\n",
    "                  done = True\n",
    "                  score -= 500\n",
    "                \n",
    "            \n",
    "\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                if not load_checkpoint:\n",
    "                    self.save_models()\n",
    "\n",
    "            #print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\n",
    "\n",
    "        if not load_checkpoint:\n",
    "          N = len(score_history)\n",
    "          running_avg = np.empty(N)\n",
    "          for t in range(N):\n",
    "            running_avg[t] = np.mean(scores[max(0, t - window):(t + 1)])\n",
    "            if x is None:\n",
    "              x = [i for i in range(N)]\n",
    "            plt.ylabel('Score')\n",
    "            plt.xlabel('Game')\n",
    "            plt.plot(x, running_avg)\n",
    "            plt.savefig(filename)\n",
    "\n",
    "    def test(self, Model_name,df):\n",
    "        self.load(Model_name)\n",
    "        for e in range(100):\n",
    "          state = self.reset(df)\n",
    "          done = False\n",
    "          score = 0\n",
    "          while not done:\n",
    "            print(done)\n",
    "            one_hot_state = preprocess(state)\n",
    "            one_hot_state = tf.convert_to_tensor([one_hot_state]) \n",
    "            prediction = self.actor_critic.predict(one_hot_state)\n",
    "            action = np.zeros(5)\n",
    "            count = 0\n",
    "            for i in prediction:\n",
    "              for j in i:\n",
    "                action[count] = np.argmax(j)\n",
    "                count +=1\n",
    "            state, reward, done, _ = self.step(action)\n",
    "            score += reward\n",
    "            print(score)\n",
    " \n",
    "          if done:\n",
    "            print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "            print(average)\n",
    "            break\n",
    "        \n",
    "      # close environemnt when finish training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rqvJgUdjtgjk",
    "outputId": "9b492a3c-10ff-4175-ec82-7745a7c49b42"
   },
   "outputs": [],
   "source": [
    "env_name = 'WAN'\n",
    "agent = Agent(env_name)\n",
    "agent.run(train)                      ## Running train sequence for agent on train set     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "OMC1u2lbu_Kc",
    "outputId": "0fafaa97-3639-40ee-861f-87f77783a471"
   },
   "outputs": [],
   "source": [
    "agent_t = Agent(\"WAN\")\n",
    "agent_t.load_models()\n",
    "agent_t.test(train)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "A2CAGENT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
